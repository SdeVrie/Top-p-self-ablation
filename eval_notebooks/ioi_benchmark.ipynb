{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import transformers\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load roneneldan/TinyStories-1M using auto model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-1M').to(device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/autocircuit_ioi_prompts.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(data['prompts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing IOI like a benchmark\n",
    "\n",
    "The initial purpose of this notebook was to use the IOI data like a benchmark to test if ablation training severely harms the performance of our model during training. The results were not as expected as even the regular tiny stories model does not fare well with how the evaluation is set up.\n",
    "\n",
    "Ultimately, while this notebook does not work, I still think we need to demonstrate that the ablation training does not harm the general capabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(prompts, tokenizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes prompts in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - prompts (list): List of prompt strings.\n",
    "    - tokenizer: The tokenizer instance.\n",
    "    - batch_size (int): Number of prompts per batch.\n",
    "\n",
    "    Returns:\n",
    "    - list of dict: Tokenized inputs for each batch.\n",
    "    \"\"\"\n",
    "    tokenized_batches = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        tokenized = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "        tokenized_batches.append(tokenized)\n",
    "    return tokenized_batches\n",
    "\n",
    "def evaluate_model_accuracy(model, tokenizer, data, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on the IOI task using batch processing.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The HuggingFace ModelForCausalLM instance.\n",
    "    - tokenizer: The corresponding tokenizer instance.\n",
    "    - data (dict): The dataset containing prompts and answers.\n",
    "    - batch_size (int): Number of prompts to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy of the model on the dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = len(data['prompts'])\n",
    "\n",
    "    # Extract all corrupt prompts and corresponding answers\n",
    "    prompts = [prompt['clean'] for prompt in data['prompts']]\n",
    "    all_answers = [[ans.strip().lower() for ans in prompt['answers']] for prompt in data['prompts']]\n",
    "\n",
    "    # Tokenize prompts in batches\n",
    "    tokenized_batches = tokenize_prompts(prompts, tokenizer, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tokenized_batches):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Generate one token per prompt in the batch\n",
    "            output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=1, do_sample=True)\n",
    "\n",
    "            # Extract the generated token IDs\n",
    "            generated_ids = output_ids[:, -1]  # Shape: (batch_size,)\n",
    "\n",
    "            # Decode the generated tokens\n",
    "            generated_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            generated_tokens = [token.strip().lower() for token in generated_tokens]\n",
    "\n",
    "            # Get the corresponding answers for the current batch\n",
    "            batch_start = idx * batch_size\n",
    "            batch_end = batch_start + len(generated_tokens)\n",
    "            batch_answers = all_answers[batch_start:batch_end]\n",
    "\n",
    "            # Compare generated tokens with answers\n",
    "            for gen_token, ans_list in zip(generated_tokens, batch_answers):\n",
    "                if gen_token in ans_list:\n",
    "                    correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.80%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model accuracy with batching\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "accuracy = evaluate_model_accuracy(model, tokenizer, data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work well since the LM uses a lot of pronouns which the answers do not account for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ablation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
